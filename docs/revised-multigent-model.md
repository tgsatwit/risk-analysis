Modular Copilot Studio Agents for CPS 230 Risk Assessment

Overview

In this implementation, six specialized Copilot Studio agents work together to execute a CPS 230 operational risk assessment for each business process. Each agent is dedicated to one step of the assessment, ensuring focused expertise, format precision, and compliance with APRA CPS 230 requirements. By separating roles, the small risk team can work on 50–100 processes in parallel, with each agent consistently producing outputs aligned to the CPS 230 report template. The agents leverage enterprise knowledge sources (e.g. SharePoint libraries of process documentation and templates) so that responses are grounded in the company’s actual process data ￼. This modular approach improves output quality by narrowing each agent’s scope and allows easy hand-off of results from one step to the next. Below, we define each agent’s purpose, system prompt, knowledge access, example usage, I/O format, and how to chain their outputs in the workflow.

Agent 1: Process Summary (Executive Overview)

Purpose: Provide a high-level executive summary of the business process, covering its objectives, flow, key decision points, and critical dependencies. This agent’s output serves as a Section 1 overview for the CPS 230 report, giving workshop participants a 5-minute read to understand the process at a glance ￼.

System Prompt:

You are an experienced operational risk SME tasked with summarizing a business process for a CPS 230 compliance workshop. In clear, concise language, describe the process’s overall purpose and objectives, how it flows from start to finish, key decision points or handoffs, any important regulatory considerations, and critical dependencies ￼. Focus on clarity and brevity – the summary should be understandable in a quick read without excessive detail. Use an authoritative, executive tone suitable for an audience of risk managers and executives. Ensure the content remains factual and aligned with the provided process documentation.

Knowledge Base & Document Access: Configure this agent to search the company’s process documentation repository for the specific process being assessed. For example, point the agent to the SharePoint site/folder containing the official procedure or process map for that business process. This allows the agent to retrieve accurate details (e.g. process scope, objectives, regulatory notes) and ground the summary in real data ￼. Optionally include a “CPS 230 guidelines” document in the knowledge base for reference on compliance terminology, though the summary should primarily reflect the process doc content.

Starter Prompts: Users can trigger this agent with friendly prompts in Copilot Studio, such as:
	•	“Give me an executive summary of the [Process Name].”
	•	“Summarize the [Process Name] process for a CPS 230 risk workshop.”
	•	“What are the key objectives and flow of [Process Name]?”

Input/Output Format: The input is typically the process name or context (and the agent will fetch the documentation). The output is 2–3 short paragraphs in Markdown (or plain text) comprising the executive summary. Keep paragraphs brief (3-5 sentences) for readability. The summary should not be a bullet list but well-structured prose. It may use subheadings or line breaks for clarity if needed, but should remain a narrative overview. Ensure terminology is consistent with company and CPS 230 language (e.g. use terms like “process step,” “control,” “risk” accurately). This output will be placed in the report’s Process Summary section, preceding the detailed process table.

Chaining Considerations: After Agent 1 produces the summary, the team can insert this text into the CPS 230 report template under the executive overview section. While Agent 2 (Process Detail Table) does not require the narrative to function, having the summary available helps ensure consistency in language and emphasis between the narrative and the upcoming detailed table. The user should save or note any key points from the summary that need to align with the table (e.g. major steps or objectives), then proceed to Agent 2 for the next step.

Agent 2: Process Detail Table (Steps, Roles, Systems, Tools)

Purpose: Generate a comprehensive Process Summary Table detailing each step of the business process and its key attributes. This agent produces Section 1’s table (or Section 2 in the report, if the executive summary is Section 1) with precise columns: Process Step, Intended Purpose, Key Activities, Key Systems, Roles/Teams, Tools & Templates, and Dependencies ￼ ￼. Its goal is to capture the end-to-end process in up to ~10 steps, in a format ready for the CPS 230 template.

System Prompt:

You are a process documentation expert Copilot. Using the official process documentation provided, produce a structured Process Summary Table for the given process. Include each major step (up to 10 steps; fewer if possible for simplicity ￼) as a row. For each step, list the Step Name, its Intended Purpose, Key Activities (the 3–5 most critical activities, as bullet points), Key Systems Used, Roles/Teams Involved, Tools/Templates/Checks used, and Key Dependencies (important inputs, outputs, or third-party links) ￼. Format the output as a Markdown table with these exact columns, preserving the order and headings. Use concise phrases and bullet lists where applicable (e.g. list activities within a cell) to maintain clarity. Ensure the table is clear and reads left-to-right with steps in logical order. Do not omit any required column, and follow the template exactly.

Knowledge Base & Document Access: This agent should be connected to the same SharePoint location or knowledge source containing the detailed process documentation (e.g. procedure manuals, process flow descriptions). It will extract specific details for each step from these documents. No external knowledge is needed beyond the process’s documentation and any uploaded templates. If the executive summary (Agent 1 output) is stored, this agent can also reference it to ensure the step list and terminology match (though the primary source is the formal document to get all roles/systems info). The SharePoint query should target the process’s content to fill in each column accurately (for example, searching for tool names or team names in the doc to populate those columns).

Starter Prompts: Example prompts for users to initiate this agent’s function:
	•	“Create a process detail table for [Process Name] as per CPS 230 template.”
	•	“List all the steps of [Process Name] with purpose, actors, systems, etc., in a table.”
	•	“Generate the summary table of key details for [Process Name].”

Input/Output Format: The input context is the specific process name or a link to its documentation. The expected output is a Markdown table that exactly matches the template structure. The first row should be the header with the column names given above (if not already fixed in the system prompt). Each subsequent row represents one process step. Within cells, maintain formatting: for example, list multiple key activities as bullet points (e.g. “• Activity A” on one line, “• Activity B” on the next, within the same cell) to mirror the template style ￼ ￼. Keep the text in each cell concise (avoid long paragraphs in cells). An example of the format is shown in the CPS 230 template:
	•	Process Step: [Step 1]
	•	Intended Purpose: [Purpose]
	•	Key Activities: • [Activity 1]  • [Activity 2]
	•	Key Systems: [Systems]
	•	Roles/Teams: [Roles]
	•	Tools & Templates: [Tools]
	•	Dependencies: [Dependencies]

Ensure that every step from start to finish is covered, and the sequence is correct. The output should fit directly into the Process Summary Table section of the report, replacing the placeholder entries ￼.

Chaining Considerations: Once Agent 2 provides the process table, the team should review it for accuracy and formatting (e.g., verify no columns are missing and bullets render properly). This table becomes a key input for Agent 3. Save the table (for example, in a Word document or as part of the draft report stored on SharePoint) so it’s accessible. In the next step, Agent 3 will build on this table ￼ to analyze failure points. The user should have the table output ready to feed into Agent 3 – either by copy-pasting it into the Agent 3 prompt or by ensuring Agent 3’s knowledge source includes the saved table file. Maintaining identical step names in this table and the subsequent analysis is crucial for consistency.

Agent 3: Failure Point Analysis

Purpose: Expand the process detail into a Failure Point Analysis, identifying what could go wrong at each step. This agent produces the section of the CPS 230 report that lists, per process step, the key potential failure scenarios, their causes, and impacts. It effectively appends additional columns to each step in the summary table: Potential Failure Points, Potential Causes, and Potential Impacts ￼. The goal is to surface 3–5 realistic failure scenarios for each step and describe why they happen and what the consequences would be, focusing on significant operational vulnerabilities.

System Prompt:

You are a risk analysis Copilot specialized in process failure modes. Taking the process steps and details provided, identify 3–5 major failure points for each step of the [Process Name]. For each failure point, give a concise description of the failure scenario, then list its potential causes (triggers for that failure) and potential impacts on the process or business outcomes ￼ ￼. Present the results as an extended table, adding columns “Failure Point,” “Potential Causes,” and “Potential Impacts” to the existing process steps. Each step from the summary table should be addressed in turn ￼. Use bullet points under Causes and Impacts if multiple points are given. Focus on meaningful, significant failures – those that could seriously disrupt the process, have happened before, or represent common risk modes ￼. Do not just list trivial issues. Keep the format structured per the CPS 230 template and keep descriptions clear and succinct.

Knowledge Base & Document Access: This agent should utilize two main sources: (1) the output of Agent 2 (the Process Detail Table) as its foundation, and (2) the original process documentation for deeper context on each step. The Process Detail Table can be provided as input (copied into the prompt or retrieved if saved on SharePoint) so the agent knows each step’s name and purpose. Additionally, any historical incident logs or risk libraries could be added to the knowledge base to help identify common failure modes (optional). Primarily, the agent will rely on the process doc to infer what could go wrong at each stage (e.g. points where handoffs occur, complex tasks, etc.). Ensure the SharePoint configuration allows searching within the process documentation for keywords like “error,” “delay,” “failure” if present, and also allow the agent to reference the summary table content so that the output stays aligned with the listed steps.

Starter Prompts: Users might start this agent with prompts like:
	•	“What are the potential failure points in each step of [Process Name]?”
	•	“Analyze [Process Name] for vulnerabilities or things that could go wrong at each step.”
	•	“Generate a failure point analysis table for [Process Name] (causes and impacts of failures by step).”

Input/Output Format: The input to Agent 3 is the context of the process steps – typically by providing the table from Agent 2 or indicating which process to analyze (since Agent 3 can fetch the table if it’s stored, or the user includes it in the prompt). The output is a Markdown table (or series of tables by step) with the columns: Process Step | Failure Point | Potential Causes | Potential Impacts ￼. Follow the exact format from the CPS 230 template’s Failure Point Analysis section. For each process step, list each failure scenario on a new row under that step. You may repeat the Step name for each failure row, or list the step once and indent subsequent failure rows, as long as it’s clear which step they belong to. In the Failure Point column, state the failure scenario briefly. In the Potential Causes and Potential Impacts columns, use bullet points if multiple causes/impacts exist ￼. For example:
	•	Step 1: [Step Name]
	•	Failure 1: [Description]
• Cause: [Cause 1]
• Impact: [Impact 1]
	•	Failure 2: [Description]
• Cause: [Cause 1]
• Impact: [Impact 1]

Ensure each listed failure is plausible and significant (avoid far-fetched scenarios). The formatting should remain clean – align bullets and use consistent terminology (e.g. if referring to roles/systems from the step, use the same names as in the step detail). This output fills the Failure Point Analysis section of the CPS 230 report, directly under each corresponding process step.

Chaining Considerations: The results of the Failure Point Analysis feed directly into the risk assessment. After Agent 3, the team should consolidate this output (likely as part of the working document). Agent 4 will use these identified failure points to formulate consolidated risks. Therefore, it’s important to preserve the linkage between failure points and their process steps (e.g., by keeping step identifiers or numbers consistent). The user should save Agent 3’s table (for example, as a section in the draft report or a separate file named “[ProcessName] Failure Analysis”) on SharePoint. When moving to Agent 4, ensure it has access to this output – the failure analysis table can be provided to Agent 4 via prompt or by the agent’s knowledge search. Also, review the failure analysis for completeness because any missing key failure might lead to missing risks in the next step.

Agent 4: Risk Consolidation and Categorization

Purpose: Translate the detailed failure points into a Consolidated Risk Register for the process. This agent distills the many failure scenarios into a focused set of 5–10 well-defined risks ￼. Each risk is articulated as a clear risk statement (event, cause, consequence) and categorized by type. The output is the Risk Register section of the CPS 230 report – a table of risks with their descriptions, categories, affected steps, and reference to the contributing failure points.

System Prompt:

You are an operational risk register expert. Based on the failure points identified for [Process Name], consolidate them into a set of distinct risks. For each risk, write a comprehensive risk statement in the format: “Risk that [event] occurs due to [causes], resulting in [consequences]” ￼. Make sure each risk statement references the context of this process. Map each risk to the relevant risk category (e.g. operational, technological, people, third-party, regulatory, etc.) ￼ and list which process steps are impacted. Also include which specific failure points (from the previous analysis) contribute to that risk. Focus on 5–10 risks total, covering all major vulnerability themes without duplication ￼ ￼. Present the output as a risk register table with columns for Risk ID, Risk Statement, Category, Steps Affected, and Contributing Failure Points.

Knowledge Base & Document Access: Agent 4 requires access to the Failure Point Analysis output (from Agent 3). This can be via an attached document or included content that lists all failure scenarios by step. The agent will parse those and group related failures into overarching risks. It may also benefit from any risk taxonomy or policy documents available (for example, a company risk category definition document or CPS 230 guidance on risk types) to correctly categorize each risk (ensuring it uses standard labels like “Operational”, “Technology”, etc., consistent with CPS 230 definitions). The SharePoint knowledge configuration should include the file or content from Agent 3’s results and possibly a reference file defining risk categories if one exists. The original process doc is less needed here, except if context is required, but primarily the agent will work off the failure analysis to form risks.

Starter Prompts: To activate this agent, a user could say:
	•	“Consolidate the failure points into a risk register for [Process Name].”
	•	“Generate a CPS 230 risk register: list top risks for [Process Name], with statements and categories.”
	•	“What are the key risks for [Process Name] based on the failure scenarios identified?”

Input/Output Format: Input will be the compiled list of failure points (Agent 3’s output) and possibly instructions on any specific risk categories to cover. The output is a Risk Register table in Markdown. Each risk should be a row with the following columns (per the template): Risk ID, Risk Statement, Risk Category, Process Steps Affected, Contributing Failure Points ￼ ￼. Use a simple identifier like R1, R2, etc., for Risk ID. The Risk Statement column contains the formulated statement (“Risk that … occurs due to …, resulting in …”). Keep these statements precise and avoid overly generic language – tie them to the process context. The Category column should use predefined categories (e.g. Operational Execution, Technology, People, Third-party, Regulatory, etc.) – ensure each risk is tagged appropriately, and collectively the risks cover a broad range of categories as applicable ￼. The Steps Affected column lists the step numbers or names impacted by the risk. The Contributing Failure Points column should reference the specific failure IDs or descriptions from Agent 3 that roll up into this risk (for traceability). For example, a row might look like:

Risk ID	Risk Statement	Risk Category	Process Steps Affected	Contributing Failure Points
R1	Risk that critical data is lost due to system outage, resulting in…	Technology	Step 3, Step 4	Step 3 – “System fails to save data”; Step 4 – “Data not transferred”

Make sure the table uses the exact column names from the template and the formatting is clean. This output populates the Consolidated Risk Register section of the CPS 230 report. The team should verify that each risk is unique (avoid duplicates) and that the number of risks is manageable (max ~10, per guidance ￼).

Chaining Considerations: After generating the risk register, the user should review it and possibly adjust any wording or categorization with their expertise. The Risk Register (Agent 4 output) is critical input for the next two agents. Save the risk register table (e.g., in the working document or as “[ProcessName] Risk Register” file on SharePoint). When proceeding to Agent 5 (Expected Controls), ensure it has access to this risk list. The user may copy the risk table into the Agent 5 prompt or the agent can retrieve it from the knowledge source. Maintaining consistent Risk IDs and names from here onward is important, as Agent 5 and 6 will refer to these risks. Also, note any particularly high-impact risks as those should definitely have strong controls in the next step.

Agent 5: Expected Controls Identification

Purpose: Propose a set of Expected Controls for each identified risk. This agent focuses on what controls should be in place to mitigate the risks from Agent 4, without yet assessing whether they exist in the current process. The output aligns with the Expected Controls portion of the CPS 230 report, outlining one or more recommended controls per risk, along with their type (preventative/detective/corrective) and category (manual/automated/semi-automated). Essentially, Agent 5 prepares the first part of the Control Analysis for each risk (the “Expected Control,” “Control Type,” and “Control Category” columns of the control table ￼), setting the stage for gap analysis in the next step.

System Prompt:

You are a controls expert Copilot. For each risk in the [Process Name] risk register provided, identify the key controls that should exist to mitigate that risk. For each risk (R1, R2, …), list the expected controls and for each control, specify the Control Type (Preventative, Detective, or Corrective) and Control Category (Manual, Automated, or Semi-automated) ￼. Include at least one preventative and one detective control for each risk wherever feasible ￼. Focus on the most important controls that would meaningfully reduce the risk (do not enumerate trivial controls or an exhaustive list ￼). Use industry best practices and the process context to suggest relevant controls (e.g. system validation, approvals, reconciliations, alarms, etc. as appropriate to the risk). Present the output structured by risk, aligning with the CPS 230 control analysis template.

Knowledge Base & Document Access: The primary input for this agent is the Risk Register from Agent 4 – it needs the list of risks and their context (risk statements and maybe categories) to generate suitable controls. Ensure the agent can see the risk register (e.g., include the markdown table from Agent 4 in the prompt, or have it saved where the agent’s knowledge search can find it). Additionally, including a controls library or policy documents in the knowledge base can be useful – for example, if the organization has an internal controls catalog or APRA guidelines on effective controls for certain risk types, the agent can draw ideas from there. However, caution the agent (via the prompt or system configuration) to only propose controls relevant to the given process context. The original process documentation may be less needed here, since we are brainstorming ideal controls, but it might help to know what existing controls are documented (so it doesn’t propose duplicates of already-existing ones – though checking existence is formally Agent 6’s job). If available, allow the agent to search any compliance frameworks or CPS 230 control requirements that align to these risks.

Starter Prompts: Users can select or type prompts such as:
	•	“For each risk in our register, list expected controls with their type and category.”
	•	“What controls should we have for the risks identified in [Process Name]?”
	•	“Generate an expected controls table for the [Process Name] risk list.”

Input/Output Format: The input provided is the set of risks (with context). The output should be structured by risk, typically as a series of mini-tables or bullet lists per risk, covering Expected Control, Control Type, and Control Category. In the final CPS 230 report, these will be integrated into the Control Analysis table. One recommended format is to present each risk as a sub-section with its controls listed, for example:

Risk R1: [Brief Risk Description]
	•	Expected Control 1: Description… (Preventative, Automated)
	•	Expected Control 2: Description… (Detective, Manual)

Risk R2: [Brief Risk Description]
	•	Expected Control 1: Description… (Preventative, Manual)

Each control line names the control and in parentheses or following text indicates its type and category. This could also be presented as a table format per risk:

Risk 1	Expected Control	Control Type	Control Category
(R1)	Control A description	Preventative	Automated
(R1)	Control B description	Detective	Manual

Repeating the risk identifier on each row helps when compiling into one table. The CPS 230 template ultimately expects a unified table grouped by risk ￼, so either format is acceptable as long as it’s easy to merge. The key is to list controls for each risk clearly, with their types/categories. Write control descriptions as concise statements (e.g. “System backup performed daily” or “Manager approval required for transactions over $X”). Do not mark any evidence or gaps in this step – we are only listing expected controls, not evaluating them. This output essentially fills out the first half of the Control Analysis section for each risk, up to the Control Category column ￼.

Chaining Considerations: After Agent 5, we have a list of recommended controls per risk. The team should review these suggestions to ensure they make sense for the business context and edit if necessary. This step is important to get right, as it defines what we will look for evidence of. Once confirmed, save the expected controls (for example, as “[ProcessName] Expected Controls” or simply keep them in the working document under each risk). These expected controls will be the input for Agent 6. In the next step, Agent 6 will take each expected control and check the actual process documentation to see if it exists or not. Therefore, it’s crucial that the risk IDs and control descriptions remain consistent when handing off to Agent 6. The user may directly feed the list of expected controls to Agent 6, or have Agent 6’s knowledge source include this output. Clear separation is maintained: Agent 5 doesn’t concern itself with whether controls exist – it acts as a forward-looking advisor, while Agent 6 will act as the checker.

Agent 6: Control Gap Analysis

Purpose: Evaluate the current process against the expected controls, identifying which controls are present, partially present, or missing, and noting any gaps. This agent completes the Control Analysis section of the CPS 230 report by adding the Process Evidence, Gap Analysis, and Next Steps information for each expected control ￼ ￼. It essentially performs a gap analysis: for each control from Agent 5, find evidence in the process documentation (if any) and then state the status (in place, partial, or no evidence) along with recommended follow-up actions for gaps.

System Prompt:

*You are an audit and controls verification Copilot. Your task is to assess each expected control for [Process Name] and determine if it exists in the current process documentation. For each expected control (grouped by risk R1, R2, etc.):
	•	Look for Process Evidence of that control in the provided process documents (e.g., specific steps, descriptions, or artifacts that indicate the control’s presence). Cite the step or document section as evidence if possible.
	•	Then perform a Gap Analysis: state one of “Control present and clearly documented,” “Control partially evident,” or “No evidence of control” for each, depending on what you find ￼.
	•	If a control is partially or not evident, suggest Next Steps to address the gap (such as further inquiry or documentation needed) ￼.
Adhere strictly to facts: do not assume a control exists if the documents don’t show it ￼. Clearly say “No evidence of control” when appropriate, rather than guessing. Ensure at least one preventative and one detective control were evaluated per risk (as expected from previous step) ￼. Present your findings in the format of the CPS 230 Control Analysis table, with columns for Process Evidence, Gap Analysis, and Next Steps added for each expected control.*

Knowledge Base & Document Access: This agent needs access to two things above all: (1) the list of Expected Controls per risk (Agent 5 output), and (2) the actual process documentation (the same docs used in Agent 1 & 2, available on SharePoint) to search for evidence of those controls. The expected controls list provides the blueprint of what to look for, and the process documents provide the content to verify against. Configure the knowledge source to include the process’s procedure documents, any controls logs or existing control catalogs related to the process, and the text of the expected controls (which could be given in the prompt or stored in a file the agent can query). The agent will likely perform searches like “approval” or “reconciliation” (depending on control names) within the process doc. Make sure it can search the relevant SharePoint site for keywords or sections. If the process documentation is large, consider pointing the agent to specific sections (if known) or splitting the doc. The agent should also be aware of the risk register context so it doesn’t lose sight of why a control exists (though primarily it checks existence). It can be helpful to include the risk ID and description alongside each expected control in the prompt for clarity.

Starter Prompts: The team can initiate this agent with prompts like:
	•	“Check the process for evidence of the expected controls for [Process Name] and identify any gaps.”
	•	“Perform a control gap analysis for [Process Name] based on our expected controls.”
	•	“For each risk’s expected controls, find if the control is present in documentation and note gaps and next steps.”

Input/Output Format: The input will be the expected controls list (by risk) and the process documentation content. The output is a detailed Control Analysis table, ideally structured by risk. In the final report, controls are usually grouped under each risk as in the template ￼. One way to format the output is:

Risk 1: [Brief Risk Description]

Expected Control	Control Type	Control Category	Process Evidence (Exists?)	Gap Analysis	Next Steps
[Control 1 Name]	Preventative	Automated	e.g., “Documented in Step 3: system generates backup report”	Control present and clearly documented	–
[Control 2 Name]	Detective	Manual	No mention in procedure	No evidence of control ￼	Investigate if this control is performed informally; review system logs.

(The above is an illustration: use actual findings.) For each expected control, populate the Process Evidence column with a brief note on what was found (or “Not found in docs” if applicable). Then fill Gap Analysis with the status phrase exactly as per CPS 230 definitions (“present and clearly documented”, “partially evident”, or “No evidence of control”) ￼. Use bold or emphasis to make “No evidence of control” stand out, if desired, as it indicates a gap. Finally, in Next Steps, provide a concise recommendation, like “Interview process owner about [Control]” or “Review XYZ configuration for evidence,” tailored to each gap ￼. If a control is present, Next Steps can be blank or “None” (or omitted). Ensure the table remains well-aligned and that each control from Agent 5 is accounted for. This output constitutes the Control Gap Analysis section of the report, completing the CPS 230 template’s last section with actionable insights.

Chaining Considerations: Agent 6 is the final step, so after its execution the team will have a completed set of report sections. It’s important that the outputs from all previous agents were accurately used here – the risk IDs and control names in this output should match those given by Agent 5 to avoid confusion. Once Agent 6 provides the control analysis table, the team should merge this into the master CPS 230 report document (likely by replacing the placeholder “Control Analysis” section with this output). Double-check consistency: for example, if a control was found to be present, does it correspond to something mentioned in the Process Summary Table or narrative? If “No evidence” is found, ensure that truly nothing in the documentation implies that control. The team might have Agent 6 run iterative queries if needed (e.g., ask it follow-up questions on any surprising gaps). Finally, the completed report for the process can be reviewed end-to-end (perhaps by a human risk manager) for quality assurance. Given that each agent was constrained to its role and used the template structure, the combined outputs should require minimal reformatting. Ensure all sections use consistent terminology (the system prompts have enforced this, e.g., using the exact phrases from the template and CPS 230 guidance across all agents). This manual chaining workflow – where users pass the documented output from one agent to the next – ensures nothing is lost in translation and maintains high precision at each step.

Best Practices and Implementation Notes

Implementing this multi-agent solution in Microsoft 365 Copilot Studio requires careful prompt design and configuration to achieve the desired precision:
	•	Clear Role Definition in Prompts: Each agent’s system prompt explicitly defines its scope and outputs. This reduces overlap and confusion. For example, Agent 3’s prompt only focuses on failure analysis and references the previous step’s table ￼, whereas Agent 4’s prompt explicitly instructs consolidating those failures into risks with a specific format ￼. By tailoring the system message to each task, we prevent a one-size-fits-all model from producing generic or inconsistent results. Keep these prompts concise yet specific – include key points from guidelines (like the risk statement format or gap rating definitions) so the agent is “aware” of the standards it must follow.
	•	Knowledge Grounding and SharePoint Setup: For reliable and factual outputs, connect each agent to relevant knowledge sources. Use SharePoint sites or folders for each process to store inputs and outputs. For instance, maintain a SharePoint library named “CPS230 Assessments” with subfolders for each business process. In each subfolder, store the process documentation, and as you complete each step, add the output (e.g., the summary table, risk register) as documents or lists. Configure the Copilot agent’s generative search to include that folder path, so it can retrieve information as needed ￼. This approach ensures the agent cites the actual process details instead of hallucinating. Make sure to publish the agents with Microsoft authentication so they can seamlessly search SharePoint on behalf of users ￼. Also, leverage any existing compliance documents (like APRA’s CPS 230 text or internal policies) as read-only reference materials in the knowledge base for terms and expectations.
	•	Formatting Accuracy: All outputs are designed to match the provided template exactly, which is critical for consistency. Agents are instructed to use the precise column headings and structure from the “CPS 230 Process Analysis Template” ￼ ￼. During testing, verify that tables render correctly – for example, bullet points within table cells and line breaks. In Markdown, this can be tricky, so instruct agents clearly on how to format lists inside cells (as we did in prompts). Where needed, the team can do minor touch-ups (e.g., if a bullet list in a cell doesn’t format perfectly when pasted to Word). However, by giving the model an exact template example, we minimize such issues. Consistently using Markdown tables and bullet lists will make it easy to copy into Word or SharePoint with minimal cleanup. Keep each paragraph short and focused, as done in the prompts, to avoid run-on outputs. The system prompts and examples serve to enforce this style.
	•	Consistency with CPS 230 Requirements: We baked CPS 230 compliance considerations directly into the agent instructions. For example, Agent 4 is told to cover a range of risk categories ￼ ensuring no major category is overlooked (a CPS 230 expectation for comprehensive risk coverage). Agent 5 and 6 incorporate APRA’s expectations around controls: at least one preventative and detective control per risk ￼, clear labeling of control effectiveness, and not assuming undocumented controls ￼. By referencing these points in the prompts, the agents’ outputs remain aligned with CPS 230 standards. It’s a best practice to include such regulatory criteria in the system prompts or as part of the knowledge base (so the model knows the compliance context). This reduces the chance of the AI going off-script or producing something non-compliant. Additionally, maintain consistent terminology (e.g., use “Risk that … occurs due to …, resulting in …” exactly for risk statements, use “No evidence of control” phrase exactly for gaps) – we reinforced this by providing the exact wording in prompts ￼.
	•	Testing and Iteration: When deploying these agents in Copilot Studio, test each agent individually with a known process to fine-tune the prompts. Check that Agent 2 doesn’t drop any column, Agent 3 finds a reasonable number of failure points, etc. If an agent’s output formatting isn’t perfect, adjust the system prompt (for example, add an explicit example row, or break down instructions). Copilot Studio allows updating the agent prompts and knowledge connections, so use that flexibility to iterate. Also, utilize the starter prompts feature in Copilot Studio to guide users – as we listed for each agent, having a few pre-defined prompt buttons helps users invoke the agent correctly without typing a long request ￼. This ensures the agent is used as designed.
	•	Parallel Workflow Management: With up to 100 processes being assessed, consider an organized approach: for instance, create a tracking sheet or Planner board where each process goes through Steps 1–6. Team members can divide and conquer, each running the agents for different processes, but following the same procedure. Because each agent is consistent, the outputs from different users will remain uniform in structure. Encourage the team to always start from Agent 1 and follow through to Agent 6 for a given process to maintain logical flow (though Agents 1 and 2 could theoretically be done in reverse order, it’s best to keep the defined order for all processes for consistency).
	•	Human Oversight and Final QA: Copilot agents dramatically speed up documentation, but human judgment is vital. After all six agents have been run for a process, a risk manager should review the combined report. They should check that the executive summary aligns with the detailed table, that each failure point indeed maps to a risk, and that the control gaps identified make sense. The modular design makes it easier to pinpoint issues (e.g., if a risk seems off, they know it originated in Agent 4’s output and can revisit that step alone). This modularity is a best practice in prompt design – it isolates errors and allows focused re-prompting if needed without redoing the entire assessment.

By following this structured guide and leveraging Copilot Studio’s features, the enterprise can deploy a reliable, high-precision multi-agent system. The result is a scalable solution where each Copilot agent serves a well-defined role in the CPS 230 operational risk assessment process, producing consistent, audit-ready documentation for compliance workshops and reviews.